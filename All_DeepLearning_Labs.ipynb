{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5doXqhyc_Rn"
      },
      "outputs": [],
      "source": [
        "#EXP 1: Analyze the impact of various activation functions on neural networkperformance in a regression task using a house price prediction dataset.\n",
        "# The objective is to assess how each activation function affects training and validation loss (MSE) to determine the most suitable function for modeling non-linear data.\n",
        "\n",
        "# STEP 1: Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.datasets import fetch_california_housing  # Import the dataset\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import warnings\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "print(\"--- Imports Complete ---\")\n",
        "\n",
        "# STEP 2: Load Dataset\n",
        "# This dataset is built into scikit-learn and will be downloaded automatically.\n",
        "housing = fetch_california_housing()\n",
        "print(f\"--- Dataset Loaded: {housing.DESCR.splitlines()[0]} ---\")\n",
        "\n",
        "# STEP 3: Feature + Target Setup\n",
        "# Create DataFrame for features\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "# Get the target variable (Median House Value)\n",
        "y = housing.target\n",
        "\n",
        "# Normalize the target variable (y) to be between 0 and 1\n",
        "y_scaler = MinMaxScaler()\n",
        "y = y_scaler.fit_transform(y.reshape(-1, 1)).ravel()\n",
        "\n",
        "# STEP 4: Preprocessing\n",
        "# The California Housing dataset is all-numeric, so we only need StandardScaler.\n",
        "scaler = StandardScaler()\n",
        "X_processed = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y,\n",
        "                                                    test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "print(f\"--- Data Processed: Training shape {X_train.shape} ---\")\n",
        "\n",
        "\n",
        "# STEP 5: Train Model Function (Optimized for Speed)\n",
        "def train_model(activation):\n",
        "    \"\"\"\n",
        "    Builds, compiles, and trains a deep neural network for regression.\n",
        "    \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(Dense(128, input_dim=X_train.shape[1],\n",
        "                    activation=activation))\n",
        "    model.add(Dense(64, activation=activation))\n",
        "    model.add(Dense(32, activation=activation))\n",
        "\n",
        "    # Output layer for regression (linear)\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # --- OPTIMIZATIONS ---\n",
        "    # Reduced epochs from 100 to 50\n",
        "    # Increased batch_size from 16 to 32\n",
        "    # This combination trains much faster.\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    # ---------------------\n",
        "\n",
        "    duration = time.time() - start\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    return duration, loss\n",
        "\n",
        "# STEP 6: Run 3 trials per activation and average results\n",
        "activations = ['relu', 'tanh', 'sigmoid']\n",
        "results = {}\n",
        "\n",
        "print(\"\\n--- Starting Model Training (3 trials per activation) ---\")\n",
        "\n",
        "for act in activations:\n",
        "    print(f\"Testing {act.upper()}...\")\n",
        "    total_time, total_loss = 0, 0\n",
        "    for i in range(3): # Run multiple trials\n",
        "        t, l = train_model(act)\n",
        "        total_time += t\n",
        "        total_loss += l\n",
        "        print(f\"  Trial {i+1}/3: MSE={l:.4f}, Time={t:.2f}s\")\n",
        "\n",
        "    results[act] = {\n",
        "        'avg_time': total_time / 3,\n",
        "        'avg_mse': total_loss / 3\n",
        "    }\n",
        "\n",
        "# STEP 7: Print Results\n",
        "print(\"\\n--- Activation Function Performance (Averaged) ---\")\n",
        "print(\"=\" * 50)\n",
        "for act in results:\n",
        "    print(f\"{act.upper():<8} | Avg. Time: {results[act]['avg_time']:>6.2f}s | Avg. MSE: {results[act]['avg_mse']:.4f}\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# STEP 8: Plot Bar Chart\n",
        "labels = list(results.keys())\n",
        "mse_vals = [results[k]['avg_mse'] for k in labels]\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "bars = plt.bar(labels, mse_vals, color=['skyblue', 'skyblue', 'skyblue'])\n",
        "\n",
        "# Highlight the best-performing (lowest MSE) activation\n",
        "min_mse = min(mse_vals)\n",
        "best_act_index = mse_vals.index(min_mse)\n",
        "bars[best_act_index].set_color('orange')\n",
        "\n",
        "plt.title(\"Average MSE for Different Activation Functions (50 Epochs)\")\n",
        "plt.ylabel(\"Mean Squared Error (MSE) - Lower is Better\")\n",
        "plt.xlabel(\"Activation Function\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 2: To train and evaluate a single-layer feedforward neural network on a real-world binary classification dataset using Stochastic Gradient Descent (SGD) and Momentum-based Gradient Descent (Momentum GD) as optimization techniques.\n",
        "# The objective is to compare and analyze the performance of both optimizers in terms of:\n",
        "#· Convergence Rate: How quickly the training loss decreases over epochs.\n",
        "#· Training Speed: The computational efficiency and time taken during training.\n",
        "#· Classification Accuracy: The predictive performance on unseen test data.\n",
        "#This study aims to highlight the impact of optimization strategy on neural network training effectiveness, particularly in low-complexity models such as single-layer networks.\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preproce ssing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer  # <-- IMPORT BUILT-IN DATASET\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- STEP 1, 2, 3: Load and Prepare Data ---\n",
        "# Load the built-in breast cancer dataset\n",
        "print(\"--- Loading Breast Cancer Dataset ---\")\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target  # 0 = malignant, 1 = benign\n",
        "\n",
        "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
        "\n",
        "# --- STEP 4: Normalize the features ---\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# --- STEP 5: Split the data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(f\"Training samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "\n",
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    # Clip z to prevent overflow in np.exp\n",
        "    z = np.clip(z, -500, 500)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Binary cross-entropy loss function\n",
        "def binary_cross_entropy(y_true, y_pred):\n",
        "    epsilon = 1e-15  # Use a smaller epsilon for more stability\n",
        "    # Clip predictions to prevent log(0)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "# Initialize weights and bias\n",
        "def initialize_weights(n_features):\n",
        "    W = np.random.randn(n_features, 1) * 0.01\n",
        "    b = 0.0\n",
        "    return W, b\n",
        "\n",
        "# --- OPTIMIZED Training function (Batch Gradient Descent) ---\n",
        "def train(X, y, optimizer='sgd', lr=0.01, epochs=100, beta=0.9):\n",
        "    n_samples, n_features = X.shape\n",
        "    W, b = initialize_weights(n_features)\n",
        "\n",
        "    # Reshape y to (n_samples, 1) for matrix operations\n",
        "    y = y.reshape(-1, 1)\n",
        "\n",
        "    loss_history = []\n",
        "    velocity_W = np.zeros_like(W)\n",
        "    velocity_b = 0.0\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # --- BATCH FORWARD PASS ---\n",
        "        # Calculate predictions for all samples at once\n",
        "        z = np.dot(X, W) + b\n",
        "        a = sigmoid(z)  # 'a' is (n_samples, 1)\n",
        "\n",
        "        # --- BATCH LOSS ---\n",
        "        loss = binary_cross_entropy(y, a)\n",
        "        loss_history.append(loss)\n",
        "\n",
        "        # --- BATCH BACKWARD PASS (GRADIENTS) ---\n",
        "        # Calculate gradients for all samples at once\n",
        "        dz = a - y  # (n_samples, 1)\n",
        "        dW = (1 / n_samples) * np.dot(X.T, dz)  # (n_features, 1)\n",
        "        db = (1 / n_samples) * np.sum(dz)      # scalar\n",
        "\n",
        "        # --- PARAMETER UPDATE ---\n",
        "        if optimizer == 'sgd':\n",
        "            W -= lr * dW\n",
        "            b -= lr * db\n",
        "\n",
        "        elif optimizer == 'momentum':\n",
        "            # Classic Polyak Momentum\n",
        "            velocity_W = (beta * velocity_W) + dW\n",
        "            velocity_b = (beta * velocity_b) + db\n",
        "\n",
        "            W -= lr * velocity_W\n",
        "            b -= lr * velocity_b\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"Trained {optimizer.upper()} for {epochs} epochs in {training_time:.4f}s\")\n",
        "    return W, b, loss_history, training_time\n",
        "\n",
        "# Prediction function\n",
        "def predict(X, W, b):\n",
        "    z = np.dot(X, W) + b\n",
        "    a = sigmoid(z)\n",
        "    return (a > 0.5).astype(int)\n",
        "\n",
        "# --- Run Experiment ---\n",
        "LR = 0.01\n",
        "EPOCHS = 200  # Increased epochs as BGD is fast\n",
        "\n",
        "# Train using SGD\n",
        "W_sgd, b_sgd, loss_sgd, time_sgd = train(X_train, y_train, optimizer='sgd', lr=LR, epochs=EPOCHS)\n",
        "\n",
        "# Train using Momentum GD\n",
        "W_mom, b_mom, loss_mom, time_mom = train(X_train, y_train, optimizer='momentum', lr=LR, epochs=EPOCHS, beta=0.9)\n",
        "\n",
        "# --- Evaluate ---\n",
        "# Predictions\n",
        "y_pred_sgd = predict(X_test, W_sgd, b_sgd)\n",
        "y_pred_mom = predict(X_test, W_mom, b_mom)\n",
        "\n",
        "# Accuracy\n",
        "acc_sgd = accuracy_score(y_test, y_pred_sgd)\n",
        "acc_mom = accuracy_score(y_test, y_pred_mom)\n",
        "\n",
        "# --- Output Results ---\n",
        "print(\"\\n--- Final Results ---\")\n",
        "print(\"Sigmoid Activation Function is being used\")\n",
        "print(f\"SGD:      Accuracy: {acc_sgd*100:.2f}% | Training Time: {time_sgd:.4f} sec | Final Loss: {loss_sgd[-1]:.4f}\")\n",
        "print(f\"Momentum: Accuracy: {acc_mom*100:.2f}% | Training Time: {time_mom:.4f} sec | Final Loss: {loss_mom[-1]:.4f}\")\n",
        "\n",
        "# Plot loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(loss_sgd, label='SGD Loss')\n",
        "plt.plot(loss_mom, label='Momentum Loss (beta=0.9)', linestyle='--')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Binary Cross-Entropy)')\n",
        "plt.title('Loss Convergence Comparison')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# --- Nuanced Conclusion ---\n",
        "print(\"\\n--- Analysis ---\")\n",
        "print(f\"Convergence: Momentum's final loss ({loss_mom[-1]:.4f}) is likely lower than SGD's ({loss_sgd[-1]:.4f}).\")\n",
        "print(f\"Speed:       Training times are similar ({time_mom:.4f}s vs {time_sgd:.4f}s) because the extra step is just simple addition.\")\n",
        "print(f\"Accuracy:    Momentum achieved {acc_mom*100:.2f}%, SGD achieved {acc_sgd*100:.2f}%.\")\n",
        "\n",
        "if acc_mom > acc_sgd and loss_mom[-1] < loss_sgd[-1]:\n",
        "    print(\"\\nConclusion: Momentum GD was the clear winner, achieving a lower loss and higher accuracy.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: The results are mixed, but Momentum generally helps converge to a better minimum.\")"
      ],
      "metadata": {
        "id": "9pMhgeI9hbea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 3: Implement and compare three advanced gradient descent optimization algorithms: Nesterov Gradient Descent, Adagrad, RMSprop and Adam—in training neural networks.\n",
        "# Design and implement a neural network to classify handwritten digits from the MNIST dataset using three advanced gradient descent optimization algorithms:\n",
        "#1. Nesterov Accelerated Gradient (NAG)\n",
        "#2. Adagrad\n",
        "#3. RMSprop\n",
        "#4. Adam\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adagrad, Adam, RMSprop\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "print(\"--- Imports Complete ---\")\n",
        "\n",
        "# --- STEP 1: Load MNIST Dataset ---\n",
        "# Keras provides MNIST as a built-in dataset\n",
        "# We use the standard 'test' set as our validation data\n",
        "(X_train, y_train), (X_val, y_val) = mnist.load_data()\n",
        "print(f\"Loaded MNIST data: {X_train.shape[0]} train samples, {X_val.shape[0]} validation samples\")\n",
        "\n",
        "# --- STEP 2: Preprocess the Data ---\n",
        "\n",
        "# Normalize pixel values from [0, 255] to [0.0, 1.0]\n",
        "X_train = X_train.astype('float32') / 255.0\n",
        "X_val = X_val.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels (e.g., 7 -> [0,0,0,0,0,0,0,1,0,0])\n",
        "y_train_cat = to_categorical(y_train, 10)\n",
        "y_val_cat = to_categorical(y_val, 10)\n",
        "print(f\"Data processed. X_train shape: {X_train.shape}, y_train_cat shape: {y_train_cat.shape}\")\n",
        "\n",
        "# --- STEP 3: Define Model Creation Function ---\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    # Add a Flatten layer to convert 28x28 images to 784-element vectors\n",
        "    model.add(Flatten(input_shape=(28, 28)))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # Output layer: 10 neurons (one for each digit) with softmax\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "# --- STEP 4: Define Optimizers ---\n",
        "results = {}\n",
        "optimizers = {\n",
        "    # Nesterov Accelerated Gradient (NAG)\n",
        "    \"NAG\": SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
        "\n",
        "    # Adagrad (Adaptive Gradient)\n",
        "    \"Adagrad\": Adagrad(learning_rate=0.01),\n",
        "\n",
        "    # RMSprop\n",
        "    \"RMSprop\": RMSprop(learning_rate=0.01),\n",
        "\n",
        "    # Adam (Adaptive Moment Estimation)\n",
        "    \"Adam\": Adam(learning_rate=0.01)\n",
        "}\n",
        "\n",
        "# --- STEP 5: Run Training Loop ---\n",
        "EPOCHS = 10  # Reduced for speed. 10 is enough to see the difference.\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "for name, opt in optimizers.items():\n",
        "    print(f\"\\n--- Training with {name} optimizer ---\")\n",
        "    model = create_model()\n",
        "\n",
        "    # Compile the model with categorical_crossentropy\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train_cat,\n",
        "        validation_data=(X_val, y_val_cat),\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        verbose=1  # Set to 1 to see epoch progress\n",
        "    )\n",
        "    results[name] = history\n",
        "\n",
        "# --- STEP 6: Plot Results ---\n",
        "def plot_results(metric):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for name, history in results.items():\n",
        "        # Plot training metric\n",
        "        plt.plot(history.history[metric], label=f'{name} Train', lw=2)\n",
        "        # Plot validation metric\n",
        "        plt.plot(history.history['val_' + metric], linestyle='--', label=f'{name} Val')\n",
        "\n",
        "    plt.title(f'Optimizer Comparison: {metric.capitalize()}', fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=12)\n",
        "    plt.ylabel(metric.capitalize(), fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot both loss and accuracy\n",
        "print(\"\\n--- Generating Plots ---\")\n",
        "plot_results('loss')\n",
        "plot_results('accuracy')"
      ],
      "metadata": {
        "id": "-RwQnUyyjFTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 4: Design and implement a Convolutional Neural Network (CNN) to classify handwritten digits from the MNIST dataset.\n",
        "#Evaluate the model performance and demonstrate how convolutional layers improve image classification accuracy compared to traditional dense networks.\n",
        "#Objective:\n",
        "#• To understand and apply CNN architecture for image classification.\n",
        "#• To evaluate the impact of convolutional layers, pooling, and activation functions.\n",
        "#• To compare CNN with a basic fully connected neural network.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "print(\"--- Imports Complete ---\")\n",
        "\n",
        "# --- STEP 1: Load and Preprocess MNIST Data ---\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(f\"Data loaded: {x_train.shape[0]} train samples, {x_test.shape[0]} test samples\")\n",
        "\n",
        "# Normalize pixel values from [0, 255] to [0.0, 1.0]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Reshape data for the CNN (add a channel dimension: 1 for grayscale)\n",
        "# Shape goes from (60000, 28, 28) to (60000, 28, 28, 1)\n",
        "x_train_cnn = np.expand_dims(x_train, -1)\n",
        "x_test_cnn = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Note: We will use 'sparse_categorical_crossentropy' as the loss function.\n",
        "# This means we can keep our labels (y_train, y_test) as simple integers (0-9)\n",
        "# and do NOT need to one-hot encode them.\n",
        "print(f\"Shape for Dense model: {x_train.shape} (uses Flatten layer)\")\n",
        "print(f\"Shape for CNN model:   {x_train_cnn.shape} (needs channel dimension)\")\n",
        "\n",
        "\n",
        "# --- STEP 2: Define and Train Baseline Dense Model (Objective 3) ---\n",
        "\n",
        "def create_dense_model():\n",
        "    model = Sequential([\n",
        "        # Flatten the 28x28 image into a 784-element vector\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "\n",
        "        # Fully connected layers\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(64, activation='relu'),\n",
        "\n",
        "        # Output layer (10 classes for 10 digits)\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print(\"\\n--- Training Basic Dense Network ---\")\n",
        "dense_model = create_dense_model()\n",
        "# The Dense model trains on the original (28, 28) data\n",
        "history_dense = dense_model.fit(x_train, y_train, epochs=5,\n",
        "                                validation_data=(x_test, y_test),\n",
        "                                verbose=1)\n",
        "dense_loss, dense_acc = dense_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# --- STEP 3: Define, Compile, and Train CNN Model (Objectives 1 & 2) ---\n",
        "\n",
        "def create_cnn_model():\n",
        "    model = Sequential([\n",
        "        # First Convolutional Layer\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Second Convolutional Layer\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "\n",
        "        # Third Convolutional Layer (as in your snippet)\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "\n",
        "        # Flatten the results to feed into a Dense layer\n",
        "        Flatten(),\n",
        "\n",
        "        # Fully connected layer\n",
        "        Dense(64, activation='relu'),\n",
        "\n",
        "        # Output layer (10 classes)\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "print(\"\\n--- Training Convolutional Neural Network (CNN) ---\")\n",
        "cnn_model = create_cnn_model()\n",
        "# The CNN trains on the reshaped (28, 28, 1) data\n",
        "history_cnn = cnn_model.fit(x_train_cnn, y_train, epochs=5,\n",
        "                            validation_data=(x_test_cnn, y_test),\n",
        "                            verbose=1)\n",
        "cnn_loss, cnn_acc = cnn_model.evaluate(x_test_cnn, y_test, verbose=0)\n",
        "\n",
        "\n",
        "# --- STEP 4: Compare Performance (Objective 3) ---\n",
        "\n",
        "print(\"\\n--- Model Performance Comparison ---\")\n",
        "print(f\"Basic Dense Network Test Accuracy: {dense_acc * 100:.2f}%\")\n",
        "print(f\"CNN Test Accuracy:               {cnn_acc * 100:.2f}%\")\n",
        "\n",
        "print(\"\\n--- Analysis ---\")\n",
        "print(\"The CNN is significantly more accurate. This is because the Conv2D layers learn\")\n",
        "print(\"spatial features (like edges, curves, and patterns) directly from the image.\")\n",
        "print(\"The Dense network, after flattening, loses all spatial information and only sees\")\n",
        "print(\"a long list of pixels, making it much harder to learn image-specific patterns.\")\n",
        "\n",
        "\n",
        "# --- STEP 5: Plot Comparison Curves ---\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot training & validation accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dense.history['val_accuracy'], label='Dense Val Accuracy', linestyle='--')\n",
        "plt.plot(history_cnn.history['val_accuracy'], label='CNN Val Accuracy', lw=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Model Accuracy Comparison')\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dense.history['val_loss'], label='Dense Val Loss', linestyle='--')\n",
        "plt.plot(history_cnn.history['val_loss'], label='CNN Val Loss', lw=2)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Model Loss Comparison')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zk-DA8H0lhEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 5: A. Image Classification Using Le Netl Neural Network (LE Net)\n",
        "#B. Image Classification Using AlexNet Neural Network\n",
        "#C. ResNet, DenseNet, and EfficientNet are all advanced convolutional neural network (CNN) architectures\n",
        "#D. Compare Le-net.Alex-net, ResNet, DenseNet, and EfficientNet\n",
        "\n",
        "\n",
        "\n",
        "# A . Le Netl\n",
        "# Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Load and Preprocess Dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() [cite: 3]\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train = x_train[..., tf.newaxis] [cite: 3]\n",
        "x_test = x_test[..., tf.newaxis] [cite: 3]\n",
        "\n",
        "# Resize MNIST to 32x32 for LeNet-5\n",
        "x_train = tf.image.resize(x_train, [32, 32]) [cite: 3]\n",
        "x_test = tf.image.resize(x_test, [32, 32]) [cite: 3]\n",
        "\n",
        "# Step 3: Build LeNet-5 Model with ReLU\n",
        "lenet5_relu = models.Sequential([ [cite: 3]\n",
        "    layers.Conv2D(6, kernel_size=(5,5), activation='relu', input_shape=(32,32,1), padding='same'), [cite: 3]\n",
        "    layers.AveragePooling2D(pool_size=(2,2), strides=2), [cite: 4]\n",
        "    layers.Conv2D(16, kernel_size=(5,5), activation='relu'), [cite: 4]\n",
        "    layers.AveragePooling2D(pool_size=(2,2), strides=2), [cite: 4]\n",
        "    layers.Flatten(), [cite: 4]\n",
        "    layers.Dense(120, activation='relu'), [cite: 4]\n",
        "    layers.Dense(84, activation='relu'), [cite: 4]\n",
        "    layers.Dense(10, activation='softmax') [cite: 4]\n",
        "])\n",
        "\n",
        "# Step 4: Compile the Model\n",
        "lenet5_relu.compile(optimizer='adam', [cite: 4]\n",
        "              loss='sparse_categorical_crossentropy', [cite: 4]\n",
        "              metrics=['accuracy']) [cite: 4]\n",
        "\n",
        "# Step 5: Train the Model\n",
        "history = lenet5_relu.fit(x_train, y_train, epochs=10, [cite: 4]\n",
        "                    validation_data=(x_test, y_test)) [cite: 4]\n",
        "\n",
        "# Step 6: Evaluate the Model\n",
        "test_loss, test_acc = lenet5_relu.evaluate(x_test, y_test) [cite: 4]\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\") [cite: 4]\n",
        "\n",
        "# Step 7: Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Acc') [cite: 4]\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc') [cite: 4]\n",
        "plt.xlabel('Epoch') [cite: 4]\n",
        "plt.ylabel('Accuracy') [cite: 4]\n",
        "plt.legend() [cite: 4]\n",
        "plt.show() [cite: 4]\n",
        "\n",
        "\n",
        "# 2.Alexnet\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load Dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() [cite: 4]\n",
        "\n",
        "# Normalize images to [0,1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 [cite: 4]\n",
        "\n",
        "# Add channel dimension\n",
        "x_train = x_train[..., tf.newaxis] [cite: 5]\n",
        "x_test = x_test[..., tf.newaxis] [cite: 5]\n",
        "\n",
        "# Reduce dataset size (use only first 20,000 train, 5,000 test samples)\n",
        "x_train, y_train = x_train[:20000], y_train[:20000] [cite: 5]\n",
        "x_test, y_test = x_test[:5000], y_test[:5000] [cite: 5]\n",
        "\n",
        "# Resize MNIST to 128x128 instead of 227x227\n",
        "x_train = tf.image.resize(x_train, [128, 128]) [cite: 5]\n",
        "x_test = tf.image.resize(x_test, [128, 128]) [cite: 5]\n",
        "\n",
        "# Step 2: Build Smaller AlexNet\n",
        "alexnet = models.Sequential([ [cite: 5]\n",
        "    layers.Conv2D(96, kernel_size=(11,11), strides=4, activation='relu', input_shape=(128,128,1)), [cite: 5]\n",
        "    layers.MaxPooling2D(pool_size=(3,3), strides=2), [cite: 5]\n",
        "    layers.Conv2D(256, kernel_size=(5,5), padding='same', activation='relu'), [cite: 5]\n",
        "    layers.MaxPooling2D(pool_size=(3,3), strides=2), [cite: 5]\n",
        "    layers.Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'), [cite: 5]\n",
        "    layers.Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'), [cite: 5]\n",
        "    layers.Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'), [cite: 5]\n",
        "    layers.MaxPooling2D(pool_size=(3,3), strides=2), [cite: 5]\n",
        "    layers.Flatten(), [cite: 5]\n",
        "    layers.Dense(1024, activation='relu'), # reduced from 4096\n",
        "    layers.Dropout(0.5), [cite: 5]\n",
        "    layers.Dense(512, activation='relu'),\t# reduced from 4096\n",
        "    layers.Dropout(0.5), [cite: 5]\n",
        "    layers.Dense(10, activation='softmax') # MNIST = 10 classes\n",
        "])\n",
        "\n",
        "# Step 3: Compile Model\n",
        "alexnet.compile(optimizer='adam', [cite: 5]\n",
        "              loss='sparse_categorical_crossentropy', [cite: 5]\n",
        "              metrics=['accuracy']) [cite: 5]\n",
        "\n",
        "# Step 4: Train (only 2 epochs)\n",
        "history = alexnet.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test)) [cite: 5]\n",
        "\n",
        "# Step 5: Evaluate\n",
        "test_loss, test_acc = alexnet.evaluate(x_test, y_test) [cite: 6]\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\") [cite: 6]\n",
        "\n",
        "# Step 6: Plot\n",
        "plt.plot(history.history['accuracy'], label='Train Acc') [cite: 6]\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc') [cite: 6]\n",
        "plt.xlabel('Epoch') [cite: 6]\n",
        "plt.ylabel('Accuracy') [cite: 6]\n",
        "plt.legend() [cite: 6]\n",
        "plt.show() [cite: 6]\n",
        "\n",
        "\n",
        "# 3.Resnet\n",
        "# Step 1: Import Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 2: Load and Preprocess Dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() [cite: 6]\n",
        "\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 [cite: 6]\n",
        "x_train = x_train[..., tf.newaxis] [cite: 6]\n",
        "x_test = x_test[..., tf.newaxis] [cite: 6]\n",
        "\n",
        "# Resize MNIST to 32x32 (ResNet expects bigger input)\n",
        "x_train = tf.image.resize(x_train, [32, 32]) [cite: 6]\n",
        "x_test = tf.image.resize(x_test, [32, 32]) [cite: 6]\n",
        "\n",
        "# Step 3: Define Residual Block\n",
        "def residual_block(x, filters, downsample=False): [cite: 6]\n",
        "    shortcut = x [cite: 6]\n",
        "    strides = 2 if downsample else 1 [cite: 6]\n",
        "\n",
        "    # First conv\n",
        "    x = layers.Conv2D(filters, (3,3), strides=strides, padding=\"same\", activation='relu')(x) [cite: 6]\n",
        "    # Second conv\n",
        "    x = layers.Conv2D(filters, (3,3), strides=1, padding=\"same\")(x) [cite: 6]\n",
        "\n",
        "    # Adjust shortcut if shape mismatch\n",
        "    if downsample or shortcut.shape[-1] != filters: [cite: 6]\n",
        "        shortcut = layers.Conv2D(filters, (1,1), strides=strides, padding=\"same\")(shortcut) [cite: 7]\n",
        "\n",
        "    # Add skip connection\n",
        "    x = layers.Add()([x, shortcut]) [cite: 7]\n",
        "    x = layers.Activation('relu')(x) [cite: 7]\n",
        "    return x [cite: 7]\n",
        "\n",
        "# Step 4: Build ResNet Model\n",
        "inputs = layers.Input(shape=(32,32,1)) [cite: 7]\n",
        "\n",
        "# Initial conv\n",
        "x = layers.Conv2D(16, (3,3), strides=1, padding=\"same\", activation='relu')(inputs) [cite: 7]\n",
        "\n",
        "# Residual blocks\n",
        "x = residual_block(x, 16) [cite: 7]\n",
        "x = residual_block(x, 16) [cite: 7]\n",
        "x = residual_block(x, 32, downsample=True) [cite: 7]\n",
        "x = residual_block(x, 32) [cite: 7]\n",
        "x = residual_block(x, 64, downsample=True) [cite: 7]\n",
        "x = residual_block(x, 64) [cite: 7]\n",
        "\n",
        "# Global average pooling and output\n",
        "x = layers.GlobalAveragePooling2D()(x) [cite: 7]\n",
        "outputs = layers.Dense(10, activation='softmax')(x) [cite: 7]\n",
        "resnet_model = models.Model(inputs, outputs) [cite: 7]\n",
        "\n",
        "# Step 5: Compile the Model\n",
        "resnet_model.compile(optimizer='adam', [cite: 7]\n",
        "                   loss='sparse_categorical_crossentropy', [cite: 7]\n",
        "                   metrics=['accuracy']) [cite: 7]\n",
        "\n",
        "# Step 6: Train the Model\n",
        "history = resnet_model.fit(x_train, y_train, epochs=10, [cite: 7]\n",
        "                     validation_data=(x_test, y_test)) [cite: 7]\n",
        "\n",
        "# Step 7: Evaluate the Model\n",
        "test_loss, test_acc = resnet_model.evaluate(x_test, y_test) [cite: 7]\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\") [cite: 7]\n",
        "\n",
        "# Step 8: Plot Accuracy\n",
        "plt.plot(history.history['accuracy'], label='Train Acc') [cite: 8]\n",
        "plt.plot(history.history['val_accuracy'], label='Val Acc') [cite: 8]\n",
        "plt.xlabel('Epochs') [cite: 8]\n",
        "plt.ylabel('Accuracy') [cite: 8]\n",
        "plt.legend() [cite: 8]\n",
        "plt.show() [cite: 8]\n",
        "\n",
        "\n",
        "\n",
        "# 4.Dense net\n",
        "# densenet_mnist.py\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Dense Block\n",
        "def dense_block(x, num_convs, growth_rate): [cite: 8]\n",
        "    for _ in range(num_convs): [cite: 8]\n",
        "        out = layers.BatchNormalization()(x) [cite: 8]\n",
        "        out = layers.ReLU()(out) [cite: 8]\n",
        "        out = layers.Conv2D(growth_rate, (3,3), padding=\"same\")(out) [cite: 8]\n",
        "        x = layers.Concatenate()([x, out]) [cite: 8]\n",
        "    return x [cite: 8]\n",
        "\n",
        "# Transition Layer\n",
        "def transition_layer(x, reduction): [cite: 8]\n",
        "    out = layers.BatchNormalization()(x) [cite: 8]\n",
        "    out = layers.ReLU()(out) [cite: 8]\n",
        "    out = layers.Conv2D(int(x.shape[-1]*reduction), (1,1))(out) [cite: 8]\n",
        "    out = layers.AveragePooling2D(pool_size=(2,2), strides=2)(out) [cite: 8]\n",
        "    return out [cite: 8]\n",
        "\n",
        "def build_densenet(input_shape=(32,32,1), num_classes=10): [cite: 8]\n",
        "    inputs = layers.Input(shape=input_shape) [cite: 8]\n",
        "\n",
        "    # Initial conv\n",
        "    x = layers.Conv2D(64, (3,3), padding=\"same\", activation=\"relu\")(inputs) [cite: 8]\n",
        "\n",
        "    # Dense Block 1\n",
        "    x = dense_block(x, num_convs=2, growth_rate=12) [cite: 8]\n",
        "    x = transition_layer(x, reduction=0.5) [cite: 8]\n",
        "\n",
        "    # Dense Block 2\n",
        "    x = dense_block(x, num_convs=2, growth_rate=12) [cite: 8]\n",
        "    x = transition_layer(x, reduction=0.5) [cite: 9]\n",
        "\n",
        "    # Dense Block 3\n",
        "    x = dense_block(x, num_convs=2, growth_rate=12) [cite: 9]\n",
        "\n",
        "    # Classification Layer\n",
        "    x = layers.BatchNormalization()(x) [cite: 9]\n",
        "    x = layers.ReLU()(x) [cite: 9]\n",
        "    x = layers.GlobalAveragePooling2D()(x) [cite: 9]\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x) [cite: 9]\n",
        "\n",
        "    model = models.Model(inputs, outputs) [cite: 9]\n",
        "    return model [cite: 9]\n",
        "\n",
        "def main(): [cite: 9]\n",
        "    # Load Dataset\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data() [cite: 9]\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0 [cite: 9]\n",
        "    x_train = x_train[..., tf.newaxis] [cite: 9]\n",
        "    x_test = x_test[..., tf.newaxis] [cite: 9]\n",
        "\n",
        "    # Resize MNIST to 32x32 for DenseNet\n",
        "    x_train = tf.image.resize(x_train, [32, 32]) [cite: 9]\n",
        "    x_test = tf.image.resize(x_test, [32, 32]) [cite: 9]\n",
        "\n",
        "    # Build DenseNet\n",
        "    densenet = build_densenet() [cite: 9]\n",
        "\n",
        "    # Compile\n",
        "    densenet.compile(optimizer='adam', [cite: 9]\n",
        "                   loss='sparse_categorical_crossentropy', [cite: 9]\n",
        "                   metrics=['accuracy']) [cite: 9]\n",
        "\n",
        "    # Train\n",
        "    history = densenet.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test)) [cite: 9]\n",
        "\n",
        "    # Evaluate\n",
        "    test_loss, test_acc = densenet.evaluate(x_test, y_test) [cite: 9]\n",
        "    print(f\"\\n ⬛  DenseNet Test Accuracy: {test_acc:.4f}\") [cite: 9]\n",
        "\n",
        "    # Plot\n",
        "    plt.plot(history.history['accuracy'], label='Train Acc') [cite: 9]\n",
        "    plt.plot(history.history['val_accuracy'], label='Val Acc') [cite: 9]\n",
        "    plt.title(\"DenseNet Training vs Validation Accuracy (MNIST)\") [cite: 9]\n",
        "    plt.xlabel('Epochs') [cite: 10]\n",
        "    plt.ylabel('Accuracy') [cite: 10]\n",
        "    plt.legend() [cite: 10]\n",
        "    plt.savefig(\"densenet_accuracy.png\") [cite: 10]\n",
        "    plt.show() [cite: 10]\n",
        "\n",
        "if __name__ == \"__main__\": [cite: 10]\n",
        "    main() [cite: 10]"
      ],
      "metadata": {
        "id": "EgZGlWLBmICs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 6:  Design and implement an Image classification model to classify a dataset of images using Deep Feed Forward NN.\n",
        "#Record the accuracy corresponding to the number of epochs. Use the MNIST datasets.\n",
        "\n",
        "# Deep Feed Forward Neural Network (DFFNN) for MNIST\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Flatten\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# 1. Load MNIST dataset\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "print(\"--- Data Loading ---\")\n",
        "print(f\"Shape of X_train: {X_train.shape}\")\n",
        "print(f\"Shape of y_train: {y_train.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "print(f\"Shape of y_test: {y_test.shape}\")\n",
        "\n",
        "# 2. Display first 10 images with labels\n",
        "print(\"\\n--- Displaying Sample Data ---\")\n",
        "fig, axs = plt.subplots(2, 5, figsize=(12, 6))\n",
        "fig.suptitle('First 10 MNIST Images', fontsize=16)\n",
        "n = 0\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        axs[i, j].imshow(X_train[n], cmap='gray')\n",
        "        axs[i, j].set_title(f\"Label: {y_train[n]}\")\n",
        "        axs[i, j].axis('off')\n",
        "        n += 1\n",
        "plt.show()\n",
        "\n",
        "# 3. Reshape (Flatten) and Normalize\n",
        "print(\"\\n--- Data Preprocessing ---\")\n",
        "# DFFNN cannot read 2D images, so we flatten 28x28 images into 784-element vectors\n",
        "X_train = X_train.reshape(60000, 784).astype(\"float32\") / 255\n",
        "X_test = X_test.reshape(10000, 784).astype(\"float32\") / 255\n",
        "print(f\"New shape of X_train (flattened): {X_train.shape}\")\n",
        "print(f\"New shape of X_test (flattened): {X_test.shape}\")\n",
        "\n",
        "# 4. Define Deep Feed Forward Neural Network\n",
        "model = Sequential(name=\"DFF-Model\")\n",
        "model.add(Input(shape=(784,), name='Input-Layer'))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer='he_normal',\n",
        "                name='Hidden-Layer-1'))\n",
        "model.add(Dense(64, activation='relu', kernel_initializer='he_normal',\n",
        "                name='Hidden-Layer-2'))\n",
        "model.add(Dense(32, activation='relu', kernel_initializer='he_normal',\n",
        "                name='Hidden-Layer-3'))\n",
        "model.add(Dense(10, activation='softmax', name='Output-Layer')) # 10 classes (0-9)\n",
        "\n",
        "# 5. Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy', # Use sparse because y_train is integers (0,1,2...)\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 6. Train model\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=10, # We will get 10 accuracy points\n",
        "                    validation_split=0.2, # Use 20% of training data for validation\n",
        "                    verbose=1)\n",
        "\n",
        "# 7. Plot accuracy vs epochs (Objective)\n",
        "print(\"\\n--- Plotting Accuracy vs Epochs ---\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Epochs\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# 8. Predictions\n",
        "# Use np.argmax to get the class index with the highest probability\n",
        "pred_labels_tr = np.argmax(model.predict(X_train), axis=1)\n",
        "pred_labels_te = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "# 9. Model Summary\n",
        "print(\"\\n--- Model Summary ---\")\n",
        "model.summary()\n",
        "\n",
        "# 10. Classification Report\n",
        "print(\"\\n---------- Evaluation on Training Data -----------\")\n",
        "print(classification_report(y_train, pred_labels_tr))\n",
        "\n",
        "print(\"\\n---------- Evaluation on Test Data (Final) -----------\")\n",
        "print(classification_report(y_test, pred_labels_te))"
      ],
      "metadata": {
        "id": "IupX_DsnnrQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 7: Implement RNN for sentiment analysis on movie reviews\n",
        "\n",
        "# RNN Sentiment Analysis on IMDB Movie Reviews\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# 1. Load IMDB dataset (top 10,000 most frequent words)\n",
        "num_words = 10000\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "print(\"--- Data Loading ---\")\n",
        "print(f\"Training samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "\n",
        "# 2. Pad sequences to have equal length (e.g., 50 words per review)\n",
        "# This ensures all input vectors have the same dimension.\n",
        "maxlen = 50\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen, padding='post')\n",
        "print(f\"Shape of X_train (padded): {X_train.shape}\")\n",
        "print(f\"Shape of X_test (padded): {X_test.shape}\")\n",
        "\n",
        "# 3. Build RNN model\n",
        "print(\"\\n--- Building Model ---\")\n",
        "model = Sequential()\n",
        "# Embedding layer: Turns word indices (e.g., 10) into dense vectors (e.g., [0.1, 0.5, ...])\n",
        "# 'num_words' = vocabulary size, 'output_dim' = vector size for each word\n",
        "model.add(Embedding(input_dim=num_words, output_dim=32,\n",
        "                    input_length=maxlen))\n",
        "\n",
        "# A SimpleRNN layer that processes the sequence of vectors\n",
        "model.add(SimpleRNN(32, return_sequences=False)) # False = only return the final output\n",
        "\n",
        "# Output layer: Sigmoid for binary classification (positive/negative)\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# 4. Compile model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 5. Train the model\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    verbose=1)\n",
        "\n",
        "# 6. Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"\\n--- Evaluation Complete ---\")\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
        "\n",
        "# 7. Plot accuracy vs epochs\n",
        "print(\"\\n--- Plotting Accuracy vs Epochs ---\")\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history['accuracy'], label=\"Training Accuracy\", marker='o')\n",
        "plt.plot(history.history['val_accuracy'], label=\"Validation Accuracy\", marker='o')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"RNN Accuracy on IMDB Sentiment Analysis\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WHZ8INjpoVf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 8: Create an RNN model that can classify the sentiment of tweets in real time.\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Imports\n",
        "# -----------------------------\n",
        "!pip install -q tensorflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json, os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Input\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "print(\"--- Imports Complete ---\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Synthetic dataset (for demo)\n",
        "# Using a tiny dataset for a quick (< 10 seconds) training example.\n",
        "# -----------------------------\n",
        "positive_examples = [\n",
        "    \"I love this!\", \"This is amazing\", \"What a fantastic day\",\n",
        "    \"So happy with the results\", \"Great job\", \"Absolutely wonderful\",\n",
        "    \"I enjoyed this a lot\", \"Highly recommend\", \"This made me smile\"\n",
        "]\n",
        "negative_examples = [\n",
        "    \"I hate this\", \"This is terrible\", \"Worst experience ever\",\n",
        "    \"So disappointed\", \"Very bad\", \"I will never use this again\",\n",
        "    \"Horrible service\", \"This ruined my day\", \"Not recommended\"\n",
        "]\n",
        "\n",
        "texts = positive_examples + negative_examples\n",
        "labels = [1]*len(positive_examples) + [0]*len(negative_examples) # 1=Positive, 0=Negative\n",
        "\n",
        "# Shuffle for training\n",
        "rng = np.random.default_rng(seed=42)\n",
        "idx = rng.permutation(len(texts))\n",
        "texts = [texts[i] for i in idx]\n",
        "labels = np.array([labels[i] for i in idx])\n",
        "\n",
        "print(f\"--- Dataset Created: {len(texts)} total samples ---\")\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Tokenization & padding\n",
        "# -----------------------------\n",
        "vocab_size = 5000      # Max words in our vocabulary\n",
        "oov_token = \"<OOV>\"    # Token for words not in the vocabulary\n",
        "max_len = 20           # Max length of a tweet/sentence\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad all sequences to be the same length (max_len)\n",
        "padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "# Split into training and validation\n",
        "split = int(0.8 * len(padded))\n",
        "x_train, x_val = padded[:split], padded[split:]\n",
        "y_train, y_val = labels[:split], labels[split:]\n",
        "\n",
        "print(f\"Training samples: {len(x_train)}, Validation samples: {len(x_val)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# 4. Build Bidirectional LSTM model\n",
        "# -----------------------------\n",
        "embedding_dim = 64\n",
        "lstm_units = 64\n",
        "dropout_rate = 0.3\n",
        "\n",
        "def build_model():\n",
        "    inp = Input(shape=(max_len,), name='input_ids')\n",
        "    # Embedding layer\n",
        "    x = Embedding(vocab_size, embedding_dim, input_length=max_len)(inp)\n",
        "    # BiLSTM reads the sequence forwards and backwards\n",
        "    x = Bidirectional(LSTM(lstm_units))(x)\n",
        "    x = Dropout(dropout_rate)(x)\n",
        "    # Output layer for binary classification\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=out)\n",
        "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "model.summary()\n",
        "\n",
        "# -----------------------------\n",
        "# 5. Train\n",
        "# -----------------------------\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "callbacks = [\n",
        "    # Stop training if 'val_loss' doesn't improve for 3 epochs\n",
        "    EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True),\n",
        "    # Save the best model found so far\n",
        "    ModelCheckpoint(\"best_model.h5\", save_best_only=True, monitor=\"val_loss\")\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    validation_data=(x_val, y_val),\n",
        "    epochs=12,\n",
        "    batch_size=4,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# 6. Plot training curves\n",
        "# -----------------------------\n",
        "print(\"\\n--- Plotting Results ---\")\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(history.history['accuracy'], label='train_acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------\n",
        "# 7. Save model & tokenizer for production\n",
        "# -----------------------------\n",
        "model.save(\"sentiment_rnn_model.keras\")\n",
        "\n",
        "# CRITICAL: Save the tokenizer so we can preprocess\n",
        "# new tweets exactly the same way as the training data.\n",
        "with open(\"tokenizer.json\", \"w\") as f:\n",
        "    f.write(tokenizer.to_json())\n",
        "\n",
        "print(f\"\\nModel saved to 'sentiment_rnn_model.keras'\")\n",
        "print(f\"Tokenizer saved to 'tokenizer.json'\")\n",
        "\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 8. \"Real-time\" prediction function\n",
        "# This part simulates a separate application\n",
        "# loading the saved files to make predictions.\n",
        "# ----------------------------------------------------\n",
        "\n",
        "print(\"\\n--- Loading Model for 'Real-Time' Prediction ---\")\n",
        "# Load the trained model\n",
        "loaded_model = load_model(\"sentiment_rnn_model.keras\")\n",
        "\n",
        "# Load and re-create the tokenizer\n",
        "with open(\"tokenizer.json\") as f:\n",
        "    tok_json = f.read()\n",
        "loaded_tokenizer = tokenizer_from_json(tok_json)\n",
        "\n",
        "def predict_tweet_sentiment(text):\n",
        "    # Preprocess the new text using the *loaded* tokenizer\n",
        "    seq = loaded_tokenizer.texts_to_sequences([text])\n",
        "    pad = pad_sequences(seq, maxlen=max_len, padding=\"post\")\n",
        "\n",
        "    # Predict\n",
        "    prob = float(loaded_model.predict(pad, verbose=0)[0][0])\n",
        "\n",
        "    # Determine label\n",
        "    label = \"positive\" if prob >= 0.5 else \"negative\"\n",
        "    return {\"text\": text, \"probability_positive\": prob, \"label\": label}\n",
        "\n",
        "# Test predictions\n",
        "print(\"\\n--- Testing 'Real-Time' Predictions ---\")\n",
        "sample_tweets = [\n",
        "    \"OMG this product is awesome, I'm so happy!\",\n",
        "    \"Totally disappointed with the service today.\",\n",
        "    \"Not sure how I feel about this.\",\n",
        "    \"Best experience ever, thank you!\",\n",
        "    \"This is the worst, will complain.\"\n",
        "]\n",
        "\n",
        "for t in sample_tweets:\n",
        "    prediction = predict_tweet_sentiment(t)\n",
        "    print(f\"Prediction: {prediction['label']} (Prob: {prediction['probability_positive']:.4f}) | Tweet: {prediction['text']}\")"
      ],
      "metadata": {
        "id": "KqSJIDgKo1Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EXP 9: Implement Auto encoders for image denoising on MNIST, Fashion, MNIST or any suitable dataset.\n",
        "\n",
        "!pip install -q tensorflow matplotlib numpy\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, losses, optimizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# ---------------- Parameters ----------------\n",
        "noise_factor = 0.5   # Amount of noise to add\n",
        "batch_size = 128\n",
        "epochs = 15          # 15 epochs is enough for a good result\n",
        "num_display = 10     # How many images to display at the end\n",
        "save_dir = \"autoencoder_denoising_output\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "print(\"--- Parameters Set ---\")\n",
        "\n",
        "# ---------------- Load & Preprocess Data ----------------\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0,1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Add channel dimension (N, 28, 28, 1)\n",
        "x_train = np.expand_dims(x_train, -1)\n",
        "x_test = np.expand_dims(x_test, -1)\n",
        "\n",
        "# Add Gaussian noise to inputs\n",
        "print(f\"--- Adding Noise (Factor: {noise_factor}) ---\")\n",
        "rng = np.random.RandomState(42)\n",
        "x_train_noisy = x_train + noise_factor * rng.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * rng.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip to keep in [0,1] range\n",
        "x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0)\n",
        "\n",
        "print(\"✅ Data prepared\")\n",
        "print(\"Training input (noisy):\", x_train_noisy.shape)\n",
        "print(\"Training target (clean):\", x_train.shape)\n",
        "\n",
        "# ---------------- Build Model ----------------\n",
        "print(\"\\n--- Building Convolutional Autoencoder ---\")\n",
        "input_img = layers.Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)  # Bottleneck -> (7, 7, 64)\n",
        "\n",
        "# Decoder\n",
        "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
        "x = layers.UpSampling2D((2, 2))(x)\n",
        "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)  # Output -> (28, 28, 1)\n",
        "\n",
        "# Define model\n",
        "autoencoder = models.Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer=optimizers.Adam(1e-3),\n",
        "                    loss=losses.MeanSquaredError())\n",
        "autoencoder.summary()\n",
        "\n",
        "# ---------------- Train Model ----------------\n",
        "print(\"\\n--- Starting Model Training ---\")\n",
        "history = autoencoder.fit(\n",
        "    x_train_noisy, x_train,  # <-- Target is the CLEAN image\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    validation_data=(x_test_noisy, x_test)\n",
        ")\n",
        "\n",
        "# ---------------- Plot Training Loss ----------------\n",
        "print(\"\\n--- Plotting Training History ---\")\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label=\"Training Loss\")\n",
        "plt.plot(history.history['val_loss'], label=\"Validation Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.legend()\n",
        "plt.title(\"Training History\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# ---------------- Predict on Noisy Test Images ----------------\n",
        "print(\"\\n--- Generating Denoised Images ---\")\n",
        "decoded_imgs = autoencoder.predict(x_test_noisy[:num_display])\n",
        "\n",
        "# ---------------- Visualization ----------------\n",
        "print(\"--- Displaying Results ---\")\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(num_display):\n",
        "    # Display Original\n",
        "    ax = plt.subplot(3, num_display, i + 1)\n",
        "    plt.imshow(x_test[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(\"Original\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display Noisy Input\n",
        "    ax = plt.subplot(3, num_display, i + 1 + num_display)\n",
        "    plt.imshow(x_test_noisy[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(\"Noisy\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display Denoised Output\n",
        "    ax = plt.subplot(3, num_display, i + 1 + 2 * num_display)\n",
        "    plt.imshow(decoded_imgs[i].squeeze(), cmap='gray')\n",
        "    ax.set_title(\"Denoised\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.suptitle(\"Autoencoder Denoising Results\", fontsize=16)\n",
        "viz_path = os.path.join(save_dir, \"mnist_denoising_result.png\")\n",
        "plt.savefig(viz_path, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ---------------- Save Model ----------------\n",
        "model_path = os.path.join(save_dir, \"autoencoder_mnist.h5\")\n",
        "autoencoder.save(model_path)\n",
        "print(\"\\n🎉 Training Complete!\")\n",
        "print(f\"📌 Saved model: {model_path}\")\n",
        "print(f\"📌 Saved visualization: {viz_path}\")\n",
        "\n",
        "# ---------------- Extra: Evaluate with PSNR & SSIM ----------------\n",
        "psnr = tf.image.psnr(x_test[:num_display], decoded_imgs, max_val=1.0)\n",
        "ssim = tf.image.ssim(x_test[:num_display], decoded_imgs, max_val=1.0)\n",
        "\n",
        "print(\"\\n--- Image Quality Metrics (on displayed samples) ---\")\n",
        "print(f\"🔎 Average PSNR (Higher is better): {np.mean(psnr.numpy()):.2f}\")\n",
        "print(f\"🔎 Average SSIM (Closer to 1 is better): {np.mean(ssim.numpy()):.4f}\")"
      ],
      "metadata": {
        "id": "-ahMXXUfpQlQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}